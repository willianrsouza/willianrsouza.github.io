{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DoomNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def perceptron(self, X, W1, bias):\n",
    "        return np.dot(X, W1) + bias\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        max = np.max(Z, axis=1, keepdims=True)\n",
    "        exp_Z = np.exp(Z - max)\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    \n",
    "    def deriv_relu(self, scores):\n",
    "        return scores > 0\n",
    "    \n",
    "    def forward_prop(self, W1, b1, W2, b2, X):\n",
    "        Z1 = X.dot(W1) + b1\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(W2) + b2\n",
    "        A2 = self.softmax(Z2)\n",
    "    \n",
    "        return Z1, A1, Z2, A2\n",
    "    \n",
    "    def backward_prop(self, Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "        m = X.shape[0]\n",
    "        error = A2 - Y\n",
    "    \n",
    "        dZ2 = error\n",
    "        dW2 = 1 / m * np.dot(A1.T, dZ2)\n",
    "        db2 = 1 / m * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        dZ1 = np.dot(dZ2, W2.T) * self.deriv_relu(Z1)\n",
    "        dW1 = 1 / m * np.dot(X.T, dZ1)\n",
    "        db1 = 1 / m * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def cross_entropy_loss(self, A2, Y):\n",
    "        m = Y.shape[0]\n",
    "        log_likelihood = -np.log(A2[range(m), Y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def update_params(self, W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "        W1 = W1 - alpha * dW1\n",
    "        b1 = b1 - alpha * db1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "    \n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def get_predictions(self, A2):\n",
    "        return np.argmax(A2, axis=1)\n",
    "    \n",
    "    def accuracy(self, Y_pred, Y_true):\n",
    "        accuracy = np.mean(Y_pred == Y_true)\n",
    "        return accuracy\n",
    "    \n",
    "    def gradient_desc(self, X, Y, iterations, alpha):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "    \n",
    "        for i in range(iterations):\n",
    "            print('Linhas:', X.shape[0])\n",
    "            Z1, A1, Z2, A2 = self.forward_prop(W1, b1, W2, b2, X)\n",
    "            dW1, db1, dW2, db2 = self.backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "            W1, b1, W2, b2 = self.update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "    \n",
    "            if i <= iterations:\n",
    "                predictions = self.get_predictions(A2)\n",
    "                acc = self.accuracy(predictions, np.argmax(Y, axis=1))\n",
    "                print(\"Iteration: \", i, \" - Accuracy: \", acc)\n",
    "\n",
    "        return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load And Prepare Data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "X_train = X_train.reshape((60000, 784))\n",
    "X_test = X_test.reshape((10000, 784))\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = DoomNet(784, 20, 10)\n",
    "model.gradient_desc(X_train, y_train, 2000, 0.20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
